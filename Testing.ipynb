{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from criteria import LadderNetLoss\n",
    "from helpers import Trainer\n",
    "from helpers import Logger\n",
    "from helpers import Trial\n",
    "from mibi_dataloader import MIBIData\n",
    "from modules import LadderNetwork\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_params\": {\n",
    "        \"kernel_size\": 3,\n",
    "        \"padding\": 0,\n",
    "        \"dilation\": 1,\n",
    "        \"stride\": 1,\n",
    "        \"in_dim\": 17,\n",
    "        \"num_layers\": 5,\n",
    "        \"code_dim\": 3,\n",
    "        \"noise_std\": 0\n",
    "    },\n",
    "    \"train_params\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 99,\n",
    "        \"epochs\": 25,\n",
    "        \"report\": 5,\n",
    "        \"crop\": 81,\n",
    "        \"clip\": 1,\n",
    "        \"decay\": 0,\n",
    "        \"error_cap\": 0.5\n",
    "    },\n",
    "    \"trial_name\": \"CCXx3xDy\",\n",
    "    \"dataset_params\": {\n",
    "        \"data_dir\": \"/home/hazmat/Documents/marchbrain/data/\",\n",
    "        \"train_dir\": \"train/\",\n",
    "        \"test_dir\": \"test/\",\n",
    "        \"crop\": 32,\n",
    "        \"scale\": 10,\n",
    "        \"stride\": 16,\n",
    "        \"train_ds_path\": \"/home/hazmat/Documents/marchbrain/hypersearch/datasets/train_ds.pickle\",\n",
    "        \"test_ds_path\": \"/home/hazmat/Documents/marchbrain/hypersearch/datasets/test_ds.pickle\"\n",
    "    },\n",
    "    \"output_params\": {\n",
    "        \"hyper_dir\": \"/home/hazmat/Documents/marchbrain/hypersearch/\",\n",
    "        \"results_dir\": \"/home/hazmat/Documents/marchbrain/hypersearch/results/\"\n",
    "    },\n",
    "    \"loss_params\": {\n",
    "        \"lambdas\": [\n",
    "            1,\n",
    "            0.5,\n",
    "            0.25,\n",
    "            0.125,\n",
    "            0.0625\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial = Trial(config, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:0 > < 13451.848595230667                                      \n",
      "Epoch:1 > < 46.47591175503201                                      \n",
      "Epoch:2 > < 14.571001243591308                                      \n",
      "Epoch:3 > < 8.328372725734004                                      \n",
      "Epoch:4 > < 4.954441881179809                                      \n",
      "Epoch:5 > < 2.992493721290871                                      \n",
      "Epoch:6 > < 1.993511755378158                                      \n",
      "Epoch:7 > < 1.636407737378721                                      \n",
      "Epoch:8 > < 1.5519415987862482                                      \n",
      "Epoch:9 > < 1.4949116238841305                                      \n",
      "Epoch:10 > < 1.4536448743608263                                      \n",
      "Epoch:11 > < 1.4056332252643726                                      \n",
      "Epoch:12 > < 1.3598544747741135                                      \n",
      "Epoch:13 > < 1.313408765969453                                      \n",
      "Epoch:14 > < 1.2604333365405047                                      \n",
      "Epoch:15 > < 1.2102484491136338                                      \n",
      "Epoch:16 > < 1.1526140734001442                                      \n",
      "Epoch:17 > < 1.0967613732373274                                      \n",
      "Epoch:18 > < 1.0501424952789589                                      \n",
      "Epoch:19 > < 1.0174917768549037                                      \n",
      "Epoch:20 > < 0.9967652757962545                                      \n",
      "Epoch:21 > < 0.9773612839204294                                      \n",
      "Epoch:22 > < 0.9689557592074076                                      \n",
      "Epoch:23 > < 0.9613528362026921                                      \n",
      "Epoch:24 > < 0.9500634458329943                                      \n",
      "trained in 622.1580510139465 seconds\n"
     ]
    }
   ],
   "source": [
    "trial.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487975233793259"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.logger.get_final_avg_loss(amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.realpath('') + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'kernel_size': 5, 'padding': 0, 'dilation': 2, 'stride': 1, 'in_dim': 17, 'num_layers': 5, 'code_dim': 3, 'noise_std': 0}, 'train_params': {'lr': 0.01, 'batch_size': 99, 'epochs': 10, 'report': 5, 'crop': 81, 'clip': 1, 'decay': 0}, 'trial_name': 'dPPovc7F', 'dataset_params': {'data_dir': '/home/hazmat/Documents/marchbrain/data/', 'train_dir': 'train/', 'test_dir': 'test/', 'crop': 32, 'scale': 10, 'stride': 16, 'train_ds_path': '/home/hazmat/Documents/marchbrain/hypersearch/datasets/train_ds.pickle', 'test_ds_path': '/home/hazmat/Documents/marchbrain/hypersearch/datasets/test_ds.pickle'}, 'output_params': {'hyper_dir': '/home/hazmat/Documents/marchbrain/hypersearch/', 'results_dir': '/home/hazmat/Documents/marchbrain/hypersearch/results/'}, 'loss_params': {'lambdas': [1, 0.5, 0.25, 0.125, 0.0625]}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# data = { \"a\":'A', \"b\":2, \"c\":3.0 }\n",
    "\n",
    "# data_json = json.dumps(data, indent=4)\n",
    "# f = open('/home/hazmat/data.json', 'w')\n",
    "# f.write(data_json)\n",
    "# f.close()\n",
    "\n",
    "with open('/home/hazmat/Documents/marchbrain/hypersearch/results/dPPovc7F/hyperpoint.json') as json_file:\n",
    "    new_data = json.load(json_file)\n",
    "    \n",
    "print(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '-la'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(['ls', '-la'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from criteria import LadderNetLoss\n",
    "from helpers import Trainer\n",
    "from helpers import Logger\n",
    "from helpers import Trial\n",
    "from mibi_dataloader import MIBIData\n",
    "from modules import LadderNetwork\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Point2.tiff', 'Point4.tiff', 'Point1.tiff', 'Point3.tiff']\n",
      "Loading.......75.0%\n",
      "There are  61504 samples\n",
      "1.7504844665527344\n"
     ]
    }
   ],
   "source": [
    "main_dir = '/home/hazmat/Documents/24Mar19_MIBI_Data_Stained_18Mar19_renamed/'\n",
    "train_dir = main_dir + 'train/'\n",
    "test_dir = main_dir + 'test/'\n",
    "modl_dir = main_dir + 'models/'\n",
    "rslt_dir = main_dir + 'results/'\n",
    "\n",
    "t = time.time()\n",
    "train_ds = MIBIData(folder=train_dir, crop=32, scale=10, stride=8)\n",
    "print(time.time()-t)\n",
    "# test_ds = MIBIData(folder=test_dir, crop=32, scale=10, stride=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hazmat/Documents/datasets/dataset.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c58a3d21e15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickling_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/hazmat/Documents/datasets/dataset.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickling_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hazmat/Documents/datasets/dataset.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickling_on = open('/home/hazmat/Documents/datasets/dataset.pickle','wb')\n",
    "pickle.dump(train_ds, pickling_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3065626621246338\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "pickle_off =  open('/home/hazmat/Documents/datasets/dataset.pickle','rb')\n",
    "reload = pickle.load(pickle_off)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hazmat/GitHub/Hawkeye/mibi_dataloader.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'x': torch.tensor(sample).float().cuda(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': tensor([0], device='cuda:0'),\n",
       " 'x': tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6534],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.6534, 0.6534, 0.6534,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.6495, 0.0000, 0.0000,  ..., 0.0000, 0.6495, 0.0000],\n",
       "           [0.6495, 0.0000, 0.6495,  ..., 0.6495, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.6495, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.6557, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6070, 0.6070],\n",
       "           [0.6070, 0.6070, 0.0000,  ..., 0.0000, 0.6070, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6070],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6070, 0.6070],\n",
       "           [0.6070, 0.6070, 0.6070,  ..., 0.6070, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.6057,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.8561, 0.5170, 0.0000,  ..., 0.5170, 0.8561, 0.0000],\n",
       "           [1.1088, 0.5170, 0.5170,  ..., 0.8561, 0.5170, 0.5170],\n",
       "           [0.8561, 0.5170, 0.5170,  ..., 0.0000, 0.5170, 0.0000],\n",
       "           ...,\n",
       "           [0.8561, 0.5170, 0.0000,  ..., 1.3104, 0.8561, 0.8561],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.5170, 0.8561, 0.8561],\n",
       "           [0.5170, 0.8561, 0.0000,  ..., 0.5170, 0.5170, 0.5170]]]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload.get_batch(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/hazmat/Documents/mayonoise/hypersearch/hypersearch.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    json_data.close()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params = data['model_params']\n",
    "print(model_params)\n",
    "new_param = [ [0,1], [1,2], [2,3] ]\n",
    "print(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def dict_prod(key, vals, dict_list):\n",
    "    dict_list_prod = []\n",
    "    for val in vals:\n",
    "        dict_list_copy = copy.deepcopy(dict_list)\n",
    "        for dictionary in dict_list_copy:\n",
    "            dictionary[key] = val\n",
    "            dict_list_prod.append(dictionary)\n",
    "    return dict_list_prod\n",
    "\n",
    "def dict_factor(dictionary):\n",
    "    dict_list = [copy.copy(dictionary)]\n",
    "    for key in dictionary:\n",
    "        vals = dictionary[key]\n",
    "        dict_list = dict_prod(key, vals, dict_list)\n",
    "    return dict_list\n",
    "\n",
    "dict_list = dict_factor(model_params)\n",
    "print(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(20):\n",
    "    time.sleep(.2)\n",
    "    print('\\r' + str(i), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "from Crypto import Random\n",
    "from Crypto.Cipher import AES\n",
    "\n",
    "class Hasher:\n",
    "    def __init__(self, key):\n",
    "        self.bs = 32\n",
    "        self.key = hashlib.sha256(key.encode()).digest()\n",
    "        \n",
    "    def encrypt(self, raw):\n",
    "        raw = self._pad(raw)\n",
    "        iv = Random.new().read(AES.block_size)\n",
    "        cipher = AES.new(self.key, AES.MODE_CBC, iv)\n",
    "        code = base64.b64encode(iv + cipher.encrypt(raw)).decode('utf8').replace('/', '_')\n",
    "        return code\n",
    "    \n",
    "    def get_names(self, dict_list):\n",
    "        name_list = list()\n",
    "        for dictionary in dict_list:\n",
    "            n = 32\n",
    "            full_name = self.encrypt(str(dictionary))\n",
    "            nick_name = full_name[0:n]\n",
    "            while nick_name in name_list:\n",
    "                n += 1\n",
    "                nick_name = full_name[0:n]\n",
    "            name_list.append(nick_name)\n",
    "        return name_list\n",
    "    \n",
    "    def _pad(self, s):\n",
    "        return s + (self.bs - len(s) % self.bs) * chr(self.bs - len(s) % self.bs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _unpad(s):\n",
    "        return s[:-ord(s[len(s)-1:])]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 'my name is Alex Baranski'\n",
    "hasher = Hasher(key)\n",
    "\n",
    "name_list = hasher.get_names(dict_list)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def print_sigmoid(model):\n",
    "    print('      weight:', model.a1.item(), model.a2.item(), model.a3.item(), model.a4.item(), model.a5.item())\n",
    "    print('    gradient:', model.a1.grad.item(), model.a2.grad.item(), model.a3.grad.item(), model.a4.grad.item(), model.a5.grad.item())\n",
    "    \n",
    "def print_param_stats(model, layer_index, kind):\n",
    "    if kind=='encoder':\n",
    "        for m in model.ladder.encoder.module_list:\n",
    "            if m.index==layer_index+1:\n",
    "                submodel = m\n",
    "        # print(submodel)\n",
    "        print('    min:', torch.min(submodel.linear.weight).item())\n",
    "        print('    max:', torch.max(submodel.linear.weight).item())\n",
    "        print('    avg:', torch.mean(submodel.linear.weight).item())\n",
    "        print('    std:', torch.std(submodel.linear.weight).item())\n",
    "    elif kind=='decoder':\n",
    "        for m in model.ladder.decoder.module_list:\n",
    "            if m.index==layer_index:\n",
    "                submodel = m\n",
    "        # print(submodel)\n",
    "        print('    min:', torch.min(submodel.linear.weight).item())\n",
    "        print('    max:', torch.max(submodel.linear.weight).item())\n",
    "        print('    avg:', torch.mean(submodel.linear.weight).item())\n",
    "        print('    std:', torch.std(submodel.linear.weight).item())\n",
    "        print_sigmoid(submodel.parsig1)\n",
    "        print_sigmoid(submodel.parsig2)\n",
    "    else:\n",
    "        print('invalid kind')\n",
    "        \n",
    "def print_layer(model, layer_index):\n",
    "    print('Encoder layer', layer_index)\n",
    "    print_param_stats(model, layer_index, 'encoder')\n",
    "    print('Decoder layer', layer_index)\n",
    "    print_param_stats(model, layer_index, 'decoder')\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "def printmem():\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    cached = torch.cuda.memory_cached()\n",
    "    print('Allocated:', str(allocated), '['+str(round(allocated/1000000000,3))+' GB]')\n",
    "    print('   Cached:', str(cached), '['+str(round(cached/1000000000,3))+' GB]')\n",
    "    \n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def print_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                print(namestr(obj, globals()), type(obj), obj.size())\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print()\n",
    "\n",
    "def printdict(dictionary):\n",
    "    keys = dictionary.keys()\n",
    "    \n",
    "    key_index = 0\n",
    "    for key in keys:\n",
    "        val = dictionary[key]\n",
    "        try:\n",
    "            if torch.is_tensor(val):\n",
    "                string = str(val.size())\n",
    "        except:\n",
    "            pass\n",
    "        if key_index == 0:\n",
    "            print('{   ' + key + ': ' + string)\n",
    "        if key_index == len(keys)-1:\n",
    "            print('    ' + key + ': ' + string + '   }')\n",
    "        else:\n",
    "            print('    ' + key + ': ' + string)\n",
    "        key_index += 1\n",
    "\n",
    "def print_tensor_dicts():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if hasattr(obj, 'keys'):\n",
    "                hastensor = False\n",
    "                for key in obj.keys():\n",
    "                    if torch.is_tensor(obj[key]):\n",
    "                        hastensor = True\n",
    "                if hastensor:\n",
    "                    printdict(obj)\n",
    "                    print()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "def print_data():\n",
    "    print_tensors()\n",
    "    print_tensor_dicts()\n",
    "    \n",
    "def deconv_out(insize, kernel_size, stride):\n",
    "    return stride*(insize-1)+kernel_size\n",
    "\n",
    "def clean_tensors(size):\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                if obj.shape[0]==size:\n",
    "                    print('Deleted object of size', obj.shape)\n",
    "                    obj = torch.zeros(1)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if hasattr(obj, 'keys'):\n",
    "                hastensor = False\n",
    "                for key in obj.keys():\n",
    "                    if torch.is_tensor(obj[key]):\n",
    "                        if obj[key].shape[0]==size:\n",
    "                            print('Deleted object of size', obj[key].shape)\n",
    "                            obj[key] = torch.zeros(1)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
